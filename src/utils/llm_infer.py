from google import genai
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List
import os
from langchain_google_genai import GoogleGenerativeAIEmbeddings

MODEL_NAME = "gemini-2.5-flash"

LLM_PROMPT = """
You are an intelligent, fast, and accurate insurance policy document parser.
Your role is to respond exactly like an experienced insurance helpdesk officer would:
clear, professional, and fully grounded in the actual policy document.

Treat the policy document as the ultimate source of truth.
Interpret the policy for clarity — do not just copy-paste legal text.
Provide a **complete answer** with all vital conditions, limits, and exceptions.
Do not bluff. If the answer is not in the document, say so directly.


---

**Guidelines for each answer:**
1. Address exactly what the question is asking in the first sentence.
2. Seamlessly integrate relevant policy clause names or numbers into your explanation.
3. Always include:
   - Eligibility conditions
   - Waiting periods
   - Applicable limits or sub-limits
   - Any key exclusions
   - Related Government Acts/Schemes if relevant
4. Use numerical digits (e.g., "30 days", "2 years", "5%") for clarity.
5. Never omit vital details even if the answer becomes longer — **complete accuracy is more important than brevity**.
6. Maintain a professional, customer-friendly tone.
7. Do not begin mechanically with "Yes" or "No". Instead, start with a natural decision statement that answers the question directly.
8. Avoid irrelevant details that are not asked.
9. Never use quotation marks or escape sequences (\n).
10. Interpret for the customer, but **keep all policy-critical conditions intact**.
11. Generate answers exactly as if an insurance helpdesk advisor is answering to customer queries on a call.
12. Refer to the company as a whole, instead of phrases like "policy is covered", use phrases like "we cover.... / we provide... / your expenses..." and other similar phrases.
13. Use phrases like your, we cover as if you genuinely are concerned and what to help the customer willingly.
14. Do not include phrases/group of words that sound mechanical or generated by an artificial chatbot. 
15. Be lively and mimick human replies.

---

**Formatting rules:**
- A single coherent paragraph per answer.
- As many words needed to cover all vital details.
- Include as many sentences as possible. Each sentence should cover each clear idea. Don't merge two sentences using conjunctions or commas.
- Filter out all quotation or escape sequences. You don't speak escape sequences while talking to customers? Do you?
- No bullet points or numbering in the output.
- Do not use symbols like & or / — write them in words.
- No currency symbols. Refer to "Table of Benefits" for amounts instead of quoting currency.

---

**Example transformation:**
Q: What is the grace period for premium payment under the National Parivar Mediclaim Plus Policy?
A: The policy allows a grace period of 30 days from the premium due date to make the payment and maintain continuity of coverage, as per clause 2.21 Grace Period. During this time, you can renew without losing waiting period or pre-existing disease coverage benefits. However, coverage is suspended until payment is received, and any claim during this period will not be payable.

---

They want:
- **Interpretation, not transcription**
- **Customer-focused framing, not policy-speak**
- **Clarity + relevance, not raw clause dumping**
"""


 


# Create a single global genai.Client instance for reuse
client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY"))

# Embedding model for vector-based search
embedding_model = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",
    google_api_key=os.getenv("GOOGLE_API_KEY"),
    task_type="RETRIEVAL_QUERY"
)

def extract_keywords(question: str) -> List[str]:
    """Extract 2-3 important keywords using the LLM."""

    keyword_prompt = f"""
    Extract not more than 2-3 important keywords from the question below. 
    These should be the high-value tokens most relevant to insurance policy documents.
    Example: "What is the waiting period of cataract surgery?" → ['waiting-period', 'cataract']

    Question: {question}
    """
    try:
        response = client.models.generate_content(
            model=MODEL_NAME,
            contents=keyword_prompt
        )
        keywords = [kw.strip() for kw in response.text.split(",") if kw.strip()]

        return keywords
    
    except Exception as e:
        print(f"[Keyword Extraction Error] {str(e)}")
        return []
    


def retrieve(question: str, keywords: List[str], vectordb, topK=5):
    try:
        q_vector = embedding_model.embed_query(question)
        results_vec = vectordb.similarity_search_by_vector(q_vector, k=topK*3)  # get more to avoid duplicates

        # Deduplicate by text content
        seen = set()
        unique_results = []
        for doc in results_vec:
            if doc.page_content not in seen:
                seen.add(doc.page_content)
                unique_results.append(doc)
            if len(unique_results) == topK:
                break

        #print("\n=== Retrieved Chunks ===")
        #for i, doc in enumerate(unique_results, 1):
        #    print(f"\n--- Chunk {i} ---")
        #    print(doc.page_content)

        return unique_results

    except Exception as e:
        print(f"[Hybrid Retrieval Error] {str(e)}")
        return []
    



def generate_answers(questions: List[str], vectordb, topK=3, num_workers=4):
    """Generate plain string answers for each question."""

    def process_single_question(question_with_index):
        index, question = question_with_index
        try:
            keywords = extract_keywords(question)
            retrieved_docs = retrieve(question, keywords, vectordb, topK=topK)
            context = "\n\n".join([doc.page_content for doc in retrieved_docs])

            # Ask Gemini without JSON formatting
            prompt = f"{LLM_PROMPT}\nContext:\n{context}\nQuestion: {question}"

            response = client.models.generate_content(
                model=MODEL_NAME,
                contents=prompt,
                config={"temperature": 0.2}
            )
            return index, response.text.strip()
        except Exception as e:
            return index, f"[Error answering question] {str(e)}"

    indexed_questions = list(enumerate(questions))
    answers = {}

    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        future_to_index = {
            executor.submit(process_single_question, q_with_idx): q_with_idx[0]
            for q_with_idx in indexed_questions
        }
        for future in as_completed(future_to_index):
            index, answer = future.result()
            answers[index] = answer

    return [answers[i] for i in range(len(questions))]